{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To support both python 2 and python 3\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "\n",
    "\n",
    "# Code to visualize the execution graph in the jupyter notebook\n",
    "from IPython.display import clear_output, Image, display, HTML\n",
    "\n",
    "def strip_consts(graph_def, max_const_size=32):\n",
    "    \"\"\"Strip large constant values from graph_def.\"\"\"\n",
    "    strip_def = tf.GraphDef()\n",
    "    for n0 in graph_def.node:\n",
    "        n = strip_def.node.add() \n",
    "        n.MergeFrom(n0)\n",
    "        if n.op == 'Const':\n",
    "            tensor = n.attr['value'].tensor\n",
    "            size = len(tensor.tensor_content)\n",
    "            if size > max_const_size:\n",
    "                tensor.tensor_content = b\"<stripped %d bytes>\"%size\n",
    "    return strip_def\n",
    "\n",
    "def show_graph(graph_def, max_const_size=32):\n",
    "    \"\"\"Visualize TensorFlow graph.\"\"\"\n",
    "    if hasattr(graph_def, 'as_graph_def'):\n",
    "        graph_def = graph_def.as_graph_def()\n",
    "    strip_def = strip_consts(graph_def, max_const_size=max_const_size)\n",
    "    code = \"\"\"\n",
    "        <script>\n",
    "          function load() {{\n",
    "            document.getElementById(\"{id}\").pbtxt = {data};\n",
    "          }}\n",
    "        </script>\n",
    "        <link rel=\"import\" href=\"https://tensorboard.appspot.com/tf-graph-basic.build.html\" onload=load()>\n",
    "        <div style=\"height:600px\">\n",
    "          <tf-graph-basic id=\"{id}\"></tf-graph-basic>\n",
    "        </div>\n",
    "    \"\"\".format(data=repr(str(strip_def)), id='graph'+str(np.random.rand()))\n",
    "\n",
    "    iframe = \"\"\"\n",
    "        <iframe seamless style=\"width:1200px;height:620px;border:0\" srcdoc=\"{}\"></iframe>\n",
    "    \"\"\".format(code.replace('\"', '&quot;'))\n",
    "    display(HTML(iframe))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_image(image,title=None):\n",
    "    plt.figure()\n",
    "    plt.imshow(image, cmap=\"gray\", interpolation=\"nearest\")\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "def plot_color_image(image):\n",
    "    plt.imshow(image.astype(np.uint8),interpolation=\"nearest\")\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "def getActivations(layer,stimuli):\n",
    "    units = sess.run(layer,feed_dict={X:np.reshape(stimuli,[1,784],order='F')})\n",
    "    plotNNFilter(units)\n",
    "\n",
    "# visualize convolution activations\n",
    "def plotNNFilter(units):\n",
    "    filters = units.shape[3]\n",
    "    plt.figure(1, figsize=(20,20))\n",
    "    n_columns = 6\n",
    "    n_rows = math.ceil(filters / n_columns) + 1\n",
    "    for i in range(filters):\n",
    "        plt.subplot(n_rows, n_columns, i+1)\n",
    "        plt.title('Filter ' + str(i))\n",
    "        plt.imshow(units[0,:,:,i], interpolation=\"nearest\", cmap=\"gray\")\n",
    "\n",
    "# visualize convolution kernels\n",
    "def plotConvLayerWeights(conv_layer_name, sess):\n",
    "    kernel = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, conv_layer_name)[0]\n",
    "    weights = sess.run(kernel)\n",
    "    print(kernel.shape)\n",
    "    \n",
    "    n_filters = int(kernel.shape[3])\n",
    "    n_base_layers = int(kernel.shape[2])\n",
    "    plt.figure(figsize=(20,20))\n",
    "    for j in range(n_base_layers):\n",
    "        for i in range(n_filters):\n",
    "            plt.subplot(n_base_layers, n_filters, j * n_filters + i + 1)\n",
    "            plt.xticks([])\n",
    "            plt.yticks([])\n",
    "            plt.imshow(weights[:,:,j,i].reshape(kernel.shape[0],kernel.shape[1]), cmap='gray', interpolation='nearest')\n",
    "            plt.title(str(i+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select 1459 and 023678 from MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_data():\n",
    "    global  dataset_1459_train, \\\n",
    "            dataset_023678_train, \\\n",
    "            X_1459_train, \\\n",
    "            y_1459_train, \\\n",
    "            X_1459_test, \\\n",
    "            y_1459_test, \\\n",
    "            X_023678_test, \\\n",
    "            y_023678_test, \\\n",
    "            X_023678_train, \\\n",
    "            y_023678_train\n",
    "    mask_1459_train = np.logical_or.reduce([mnist.train.labels == v for v in [1,4,5,9]])\n",
    "    mask_1459_test  = np.logical_or.reduce([mnist.test.labels  == v for v in [1,4,5,9]])\n",
    "    X_1459_train = np.compress(np.array(mask_1459_train), mnist.train.images, axis=0)\n",
    "    _, y_1459_train = np.unique(mnist.train.labels[mask_1459_train], return_inverse=True) # 1,4,5,9 to 0,1,2,3\n",
    "    dataset_1459_train = tf.contrib.data.Dataset.from_tensor_slices((X_1459_train,y_1459_train))\n",
    "\n",
    "    X_1459_test = np.compress(np.array(mask_1459_test), mnist.test.images, axis=0)\n",
    "    _, y_1459_test = np.unique(mnist.test.labels[mask_1459_test], return_inverse=True)\n",
    "\n",
    "    mask_023678_train = np.logical_or.reduce([mnist.train.labels == v for v in [0,2,3,6,7,8]])\n",
    "    mask_023678_test  = np.logical_or.reduce([mnist.test.labels  == v for v in [0,2,3,6,7,8]])\n",
    "    X_023678_train = np.compress(np.array(mask_023678_train), mnist.train.images, axis=0)\n",
    "    _, y_023678_train = np.unique(mnist.train.labels[mask_023678_train], return_inverse=True) # 1,4,5,9 to 0,1,2,3\n",
    "    dataset_023678_train = tf.contrib.data.Dataset.from_tensor_slices((X_023678_train,y_023678_train))\n",
    "\n",
    "    X_023678_test = np.compress(np.array(mask_023678_test), mnist.test.images, axis=0)\n",
    "    _, y_023678_test = np.unique(mnist.test.labels[mask_023678_test], return_inverse=True)\n",
    "\n",
    "prepare_data()\n",
    "\n",
    "# for example\n",
    "plot_image(X_1459_test[2].reshape(28,28),title=y_1459_test[2])\n",
    "plot_image(X_023678_test[2].reshape(28,28),title=y_023678_test[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "height = 28\n",
    "width = 28\n",
    "channels = 1\n",
    "n_inputs = height * width\n",
    "n_outputs = 4\n",
    "\n",
    "reset_graph()\n",
    "prepare_data()\n",
    "\n",
    "# inputs layer\n",
    "with tf.name_scope(\"inputs\"):\n",
    "    X = tf.placeholder(tf.float32, shape=[None, n_inputs], name=\"X\")\n",
    "    X_reshaped = tf.reshape(X, shape=[-1, height, width, channels])\n",
    "    y = tf.placeholder(tf.int32, shape=[None], name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"conv\"):\n",
    "    # conv1 = tf.layers.conv2d(X_reshaped, ...)\n",
    "    # conv2 = tf.layers.conv2d(conv1, ...)\n",
    "\n",
    "with tf.name_scope(\"pool3\"):\n",
    "    # pool3 = tf.nn.max_pool(conv2, ...)\n",
    "    # pool3_flat = tf.reshape(...)\n",
    "\n",
    "with tf.name_scope(\"fc1\"):\n",
    "    # fc1 = tf.layers.dense(pool3_flat, ...)\n",
    "\n",
    "with tf.name_scope(\"fc2\"):\n",
    "    # fc2 = tf.layers.dense(fc1, ...)\n",
    "\n",
    "with tf.name_scope(\"output\"):\n",
    "    # logits = tf.layers.dense(fc2, ...)\n",
    "    # Y_proba = tf.nn.softmax(logits, ...)\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=y)\n",
    "    loss = tf.reduce_mean(xentropy)\n",
    "    optimizer = tf.train.AdamOptimizer()\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "with tf.name_scope(\"init_and_save\"):\n",
    "    init = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train 1459"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_epochs = 5\n",
    "batch_size = 10\n",
    "\n",
    "print(\"Training...\")\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    batched_dataset = dataset_1459_train.batch(batch_size)\n",
    "\n",
    "    iterator = batched_dataset.make_initializable_iterator()\n",
    "    next_batch = iterator.get_next()\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        sess.run(iterator.initializer)\n",
    "        while True:\n",
    "            try:\n",
    "                X_batch, y_batch = sess.run(next_batch)\n",
    "                sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                # finished running through dataset\n",
    "                break\n",
    "\n",
    "        acc_train = accuracy.eval(feed_dict={X: X_1459_train, y: y_1459_train})\n",
    "        acc_test = accuracy.eval(feed_dict={X: X_1459_test, y: y_1459_test})\n",
    "        print(epoch, \"Train accuracy:\", acc_train, \"Test accuracy:\", acc_test)\n",
    "        \n",
    "    print(\"Finished training\")\n",
    "\n",
    "    print(\"Saving...\")\n",
    "    save_path = saver.save(sess, \"./my_model_1459.ckpt\")\n",
    "        \n",
    "    # for example\n",
    "    inferred = sess.run(Y_proba, feed_dict={X: [X_1459_test[0]]})\n",
    "    plot_image(X_1459_test[0].reshape(28,28), \"Predicted %d, truth %d\"%(np.argmax(inferred), y_1459_test[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the graph for 023678"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "prepare_data()\n",
    "\n",
    "# restore the graph of 1459\n",
    "restore_saver = tf.train.import_meta_graph(\"./my_model_1459.ckpt.meta\")\n",
    "\n",
    "# reuse the inputs (X,y)\n",
    "X = tf.get_default_graph().get_tensor_by_name(\"inputs/X:0\")\n",
    "y = tf.get_default_graph().get_tensor_by_name(\"inputs/y:0\")\n",
    "# reuse the FC1 layer\n",
    "fc1 = tf.get_default_graph().get_tensor_by_name(\"fc1/fc1/Relu:0\")\n",
    "# reuse the FC2 layer\n",
    "fc2_reuse = tf.get_default_graph().get_tensor_by_name(\"fc2/fc2/Relu:0\")\n",
    "\n",
    "# continue the 023678 graph from FC2...\n",
    "with tf.name_scope(\"fc2_023678\"):\n",
    "    # fc2 = tf.layers.dense(fc1, ..., name=\"fc2_023678\")\n",
    "\n",
    "with tf.name_scope(\"softmax_023678\"):\n",
    "    # logits = tf.layers.dense(fc2, ..., name=\"output_023678\")\n",
    "    # Y_proba = tf.nn.softmax(logits, name=\"Y_proba_023678\")\n",
    "\n",
    "with tf.name_scope(\"train_023678\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=y)\n",
    "    loss = tf.reduce_mean(xentropy)\n",
    "    optimizer = tf.train.AdamOptimizer(name=\"adam_023678\")\n",
    "## try to freeze everything but fc2, softmax\n",
    "#     train_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"fc2_023678|softmax_023678\")\n",
    "#     training_op = optimizer.minimize(loss, var_list=train_vars)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "with tf.name_scope(\"eval_023678\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "with tf.name_scope(\"init_023678\"):\n",
    "    init = tf.global_variables_initializer()\n",
    "    new_saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train transferred 023678"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_epochs = 3\n",
    "batch_size = 10\n",
    "\n",
    "print(\"Training 023678...\")\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    batched_dataset = dataset_023678_train.batch(batch_size)\n",
    "\n",
    "    iterator = batched_dataset.make_initializable_iterator()\n",
    "    next_batch = iterator.get_next()\n",
    "    \n",
    "    saver.restore(sess, \"./my_model_1459.ckpt\")\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        sess.run(iterator.initializer)\n",
    "        while True:\n",
    "            try:\n",
    "                X_batch, y_batch = sess.run(next_batch)\n",
    "                sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                # finished running through dataset\n",
    "                break\n",
    "\n",
    "        acc_train = accuracy.eval(feed_dict={X: X_023678_train, y: y_023678_train})\n",
    "        acc_test = accuracy.eval(feed_dict={X: X_023678_test, y: y_023678_test})\n",
    "        print(epoch, \"Train accuracy:\", acc_train, \"Test accuracy:\", acc_test)\n",
    "        \n",
    "    print(\"Finished training\")\n",
    "\n",
    "    # for example\n",
    "    inferred = sess.run(Y_proba, feed_dict={X: [X_023678_test[0]]})\n",
    "    plot_image(X_023678_test[0].reshape(28,28), \"Predicted %d, truth %d\"%(np.argmax(inferred), y_023678_test[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the Feature Maps\n",
    "The following code is useful to look into the activations of convolutional and pooling layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    saver.restore(sess, \"./my_model_1459.ckpt\")\n",
    "    # getActivations(tf.get_default_graph().get_tensor_by_name(...), X_1459_test[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the convolution kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"./my_model_1459.ckpt\")\n",
    "    \n",
    "    # plotConvLayerWeights(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show the Execution Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "# restore the graph of 1459\n",
    "restore_saver = tf.train.import_meta_graph(\"./my_model_1459.ckpt.meta\")\n",
    "\n",
    "show_graph(tf.get_default_graph())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
